{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS =1\n",
    "CONV1_SIZE = 5\n",
    "CONV1_KERNAL_NUM = 32\n",
    "CONV2_SIZE =5\n",
    "CONV2__KERNAL_NUM = 64\n",
    "FC_SIZE = 512\n",
    "OUTPUT_NODE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(shape,regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev = 0.1))\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,w):\n",
    "    return tf.nn.conv2d(x,w,strides = [1,1,1,1], padding= 'SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, train,regularizer):\n",
    "    conv1_w = get_weight([CONV1_SIZE,CONV1_SIZE,NUM_CHANNELS,CONV1_KERNAL_NUM],regularizer)\n",
    "    conv1_b = get_bias(CONV1_KERNAL_NUM)\n",
    "    conv1 = conv2d(x,conv1_w)\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1,conv1_b))\n",
    "    pool1 = max_pool_2x2(relu1)\n",
    "    \n",
    "    conv2_w = get_weight([CONV2_SIZE,CONV2_SIZE,CONV1_KERNAL_NUM,CONV2_KERNAL_NUM],regularizer)\n",
    "    conv2_b = get_bias(CONV2_KERNAL_NUM)\n",
    "    conv2 = conv2d(pool1,conv2_w)\n",
    "    relu2 = tf.nn.relu(conv2+conv2_b)\n",
    "    pool2 = max_pool_2x2(relu2)\n",
    "    # change the 3d tensor to 2d tensor, and feed the data to the FC layer.\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped = tf.reshape(pool2,[pool_shape[0],nodes])\n",
    "    \n",
    "    fc1_w = get_weight([nodes,FC_SIZE],regularizer)\n",
    "    fc1_b = get_bias([FC_SIZE])\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_w)+fc1_b)\n",
    "    if train: \n",
    "        fc1 = tf.nn.dropout(fc1,0.5)\n",
    "        \n",
    "    fc2_w = get_weight([FC_SIZE,OUTPUT_NODE],regularizer)\n",
    "    fc2_b = get_bias([OUTPUT_NODE])\n",
    "    y = tf.matmul(fc1,fc2_w)+fc2_b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.005\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZER = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = \"./lenet5/model\"\n",
    "MODEL_NAME = 'mnist_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./data/',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(mnist):\n",
    "    x = tf.placeholder(tf.float32,shape = [BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS])\n",
    "    y = tf.placeholder(tf.float32,shape = [BATCH_SIZE,OUTPUT_NODE])\n",
    "    y_ = forward(x,True,REGULARIZER)\n",
    "    \n",
    "    global_step = tf.Variable(0,trainable =False)\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y_, labels = tf.argmax(y,1))\n",
    "    cem =tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(learning_rate = LEARNING_RATE_BASE,global_step = global_step,decay_steps = mnist.train.num_examples/BATCH_SIZE,decay_rate = LEARNING_RATE_DECAY,staircase = True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step = global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(decay = MOVING_AVERAGE_DECAY,num_updates = global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshaped_xs = np.reshape(xs,[BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS])\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:reshaped_xs,y:ys})\n",
    "            \n",
    "            \n",
    "            if i%100 ==0:\n",
    "                print('After %d training steps, loss on training batch is %g'%(step,loss_value))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step = global_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lenet5/model/mnist_model-0\n",
      "After 1 training steps, loss on training batch is 1.05272\n",
      "After 101 training steps, loss on training batch is 1.027\n",
      "After 201 training steps, loss on training batch is 1.06863\n",
      "After 301 training steps, loss on training batch is 1.00309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/selector_events.py\", line 130, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ValueError: embedded null byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 401 training steps, loss on training batch is 0.993359\n",
      "After 501 training steps, loss on training batch is 0.840061\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "After 601 training steps, loss on training batch is 1.0009\n",
      "After 701 training steps, loss on training batch is 0.877063\n",
      "After 801 training steps, loss on training batch is 0.962802\n",
      "After 901 training steps, loss on training batch is 0.863011\n",
      "After 1001 training steps, loss on training batch is 0.908407\n",
      "After 1101 training steps, loss on training batch is 0.866983\n",
      "After 1201 training steps, loss on training batch is 0.816147\n",
      "After 1301 training steps, loss on training batch is 1.00875\n",
      "After 1401 training steps, loss on training batch is 0.926187\n",
      "After 1501 training steps, loss on training batch is 0.807646\n",
      "After 1601 training steps, loss on training batch is 0.843769\n",
      "After 1701 training steps, loss on training batch is 0.802828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseSelectorEventLoop._read_from_self()\n",
      "handle: <Handle BaseSelectorEventLoop._read_from_self()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/selector_events.py\", line 130, in _read_from_self\n",
      "    data = self._ssock.recv(4096)\n",
      "ValueError: embedded null byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1801 training steps, loss on training batch is 0.894504\n",
      "After 1901 training steps, loss on training batch is 0.908394\n",
      "After 2001 training steps, loss on training batch is 0.791468\n",
      "After 2101 training steps, loss on training batch is 0.807414\n",
      "After 2201 training steps, loss on training batch is 0.800616\n",
      "After 2301 training steps, loss on training batch is 0.90492\n",
      "After 2401 training steps, loss on training batch is 0.781209\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "backward(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
